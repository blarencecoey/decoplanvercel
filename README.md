<img width="460.8" height="307.2" alt="DecoPLanLLM" src="assets/DecoPLanLLM.png" />

# DecoPlan LLM

AI-powered interior design assistant for Singapore BTOs and HDB layouts using multimodal vision-language models with **RAG (Retrieval-Augmented Generation)** and **LoRA (Low-Rank Adaptation)**.

## ğŸš€ Quick Start

### Backend Setup (Python)
```bash
# Install dependencies
pip install -r backend/requirements.txt

# Run setup script
bash scripts/setup/setup_rag_lora.sh

# Start API server
bash scripts/deployment/start_backend.sh
```

### Frontend Setup (React)
```bash
cd frontend
npm install
npm run dev
```

### C++ Inference
```bash
cd cpp
mkdir build && cd build
cmake .. -DDECOPLAN_USE_CUDA=ON
make -j$(nproc)
```

## ğŸ“‹ Features

### ğŸ Backend (Python)
- **RAG System**: Semantic search over 10,000+ furniture items
- **LoRA Fine-tuning**: Efficient model adaptation for HDB-specific design
- **Flask API**: REST endpoints for furniture recommendations
- **Fast Retrieval**: <1 second query time on CPU

### âš›ï¸ Frontend (React + TypeScript)
- **Modern UI**: Built with React, TypeScript, and Tailwind CSS
- **Real-time Search**: Instant furniture recommendations
- **Advanced Filtering**: Filter by furniture type, style, material, color
- **Responsive Design**: Works on desktop and mobile

### ğŸ”§ C++ Inference Engine
- **Vision-Language Models**: LLaVA, Qwen2-VL, Llama 3.2 Vision
- **CUDA Acceleration**: GPU-optimized inference
- **GGUF Support**: Quantized models (Q4_K_M, Q5_K_M)
- **Streaming**: Real-time token generation

## ğŸ“ Project Structure

```
DecoPlan-LLM/
â”œâ”€â”€ backend/              # ğŸ Python Backend
â”‚   â”œâ”€â”€ api/             # Flask REST API
â”‚   â”œâ”€â”€ rag/             # Retrieval-Augmented Generation
â”‚   â”œâ”€â”€ lora/            # LoRA training & fine-tuning
â”‚   â””â”€â”€ tests/           # Backend tests
â”‚
â”œâ”€â”€ frontend/            # âš›ï¸ React Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/  # UI components
â”‚   â”‚   â”œâ”€â”€ services/    # API services
â”‚   â”‚   â””â”€â”€ types/       # TypeScript types
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ cpp/                 # ğŸ”§ C++ Inference Engine
â”‚   â”œâ”€â”€ include/         # Header files
â”‚   â”œâ”€â”€ src/             # Source files
â”‚   â”œâ”€â”€ examples/        # Example programs
â”‚   â””â”€â”€ CMakeLists.txt   # Build configuration
â”‚
â”œâ”€â”€ data/                # ğŸ“Š Data & Databases
â”‚   â”œâ”€â”€ datasets/        # Furniture datasets (CSV)
â”‚   â””â”€â”€ furniture_db/    # Vector database (ChromaDB)
â”‚
â”œâ”€â”€ docs/                # ğŸ“š Documentation
â”‚   â”œâ”€â”€ setup/           # Installation & setup guides
â”‚   â”œâ”€â”€ backend/         # Backend documentation
â”‚   â”œâ”€â”€ cpp/             # C++ build & usage
â”‚   â””â”€â”€ api/             # API documentation
â”‚
â”œâ”€â”€ scripts/             # ğŸ› ï¸ Utility Scripts
â”‚   â”œâ”€â”€ setup/           # Setup & installation scripts
â”‚   â”œâ”€â”€ deployment/      # Deployment scripts
â”‚   â”œâ”€â”€ data/            # Data processing scripts
â”‚   â””â”€â”€ models/          # Model download scripts
â”‚
â”œâ”€â”€ models/              # ğŸ¤– Model Files
â”‚   â””â”€â”€ *.gguf          # Quantized GGUF models
â”‚
â”œâ”€â”€ config/              # âš™ï¸ Configuration
â”‚   â”œâ”€â”€ .clang-format
â”‚   â””â”€â”€ .vscode/
â”‚
â””â”€â”€ external/            # ğŸ“¦ External Dependencies
    â””â”€â”€ llama.cpp/       # llama.cpp submodule
```

## ğŸ“š Documentation

- **[Migration Guide](MIGRATION_GUIDE.md)** - Adapting to the new structure
- **[Setup Guides](docs/setup/)** - Installation and quickstart
- **[Backend Docs](docs/backend/)** - RAG & LoRA system documentation
- **[C++ Docs](docs/cpp/)** - Build and usage instructions
- **[API Docs](docs/api/)** - REST API reference

## ğŸ¯ Usage Examples

### Python Backend

**RAG Furniture Search:**
```bash
python -m backend.rag.furniture_retriever --query "modern minimalist sofa"
```

**Build Vector Database:**
```bash
python -m backend.rag.build_furniture_db
```

**Train LoRA Adapter:**
```bash
python -m backend.lora.train_lora
```

**Start API Server:**
```bash
cd backend/api
python app.py
```

### C++ Inference

```bash
cd cpp/build
./multimodal_inference \
    ../../models/llava-v1.6-mistral-7b.Q4_K_M.gguf \
    floor_plan.jpg \
    "Suggest furniture placement for this living room"
```

### Frontend

```bash
cd frontend
npm run dev    # Development server
npm run build  # Production build
```

## ğŸ”§ System Requirements

### Backend (Python)
- **OS**: Linux, macOS, Windows (WSL2)
- **Python**: 3.8+
- **RAM**: 8GB minimum, 16GB recommended
- **Dependencies**: See `backend/requirements.txt`

### Frontend (React)
- **Node.js**: 18.0+
- **npm**: 9.0+

### C++ Inference
- **OS**: Linux, macOS, Windows (WSL2)
- **CPU**: x86_64 with AVX2 or ARM64
- **GPU** (Optional): NVIDIA GPU with CUDA 11.0+, or Apple Silicon (Metal)
- **VRAM**: 6GB minimum for Q4_K_M quantization
- **RAM**: 16GB minimum, 32GB recommended

## ğŸš¢ Deployment

### Backend API
```bash
# Development
bash scripts/deployment/start_backend.sh

# Production (with gunicorn)
cd backend/api
gunicorn -w 4 -b 0.0.0.0:5000 app:app
```

### Frontend
```bash
cd frontend
npm run build
# Deploy the 'dist' folder to your hosting service
```

## ğŸ› ï¸ Development

### Running Tests
```bash
# Backend tests
cd backend
python -m pytest tests/

# Frontend tests
cd frontend
npm test
```

### Code Formatting
```bash
# C++ (uses .clang-format)
cd cpp
clang-format -i src/*.cpp include/*.h

# Python (uses black)
cd backend
black .

# TypeScript/React (uses prettier)
cd frontend
npm run format
```

## ğŸ“Š Performance

| Component | Metric | Value |
|-----------|--------|-------|
| RAG Search | Query Time | <1 second (CPU) |
| RAG Database | Items | 10,000+ furniture pieces |
| C++ Inference | Model Size | 4-8GB (Q4_K_M) |
| C++ Inference | VRAM Usage | 6-8GB (7B model) |
| LoRA Training | Parameters | 1-2% trainable |
| LoRA Training | Time | 2-4 hours (12GB+ VRAM) |

## ğŸ¤ Contributing

This is a university project for HDB/BTO interior design assistance. Contributions welcome!

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and formatting
5. Submit a pull request

## ğŸ“ License

See [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- Built on [llama.cpp](https://github.com/ggerganov/llama.cpp) by Georgi Gerganov
- Models from [Hugging Face](https://huggingface.co/) community
- Inspired by Singapore HDB interior design needs
- Vector database powered by [ChromaDB](https://www.trychroma.com/)
- Frontend UI components from [shadcn/ui](https://ui.shadcn.com/)

## ğŸ“ Support

- **Documentation**: Check `docs/` directory
- **Migration Issues**: See [MIGRATION_GUIDE.md](MIGRATION_GUIDE.md)
- **Bug Reports**: Open an issue on GitHub

---

**Made with â¤ï¸ for Singapore HDB/BTO Interior Design**
